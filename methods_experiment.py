# -*- coding: utf-8 -*-
"""Methods_Experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xhB_BBNgcLvWSD9b7GMPEuovzU0A3CqY
"""

!pip install -U scikit-learn pandas openpyxl transformers torch

import pandas as pd
import numpy as np
import re
import string
import torch
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    ConfusionMatrixDisplay,
)

df = pd.read_excel("Cleaned_Data.xlsx")
print(df.shape)
print(df.columns)
df.head()

TEXT_COL = "text"
LABEL_COL = "label"
x = df[TEXT_COL].astype(str)
y = df[LABEL_COL].astype(str)
y.value_counts()

# First: train+val vs test
x_tv, x_test, y_tv, y_test = train_test_split(
    x, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

# Then: split train+val into train and val
x_train, x_val, y_train, y_val = train_test_split(
    x_tv, y_tv,
    test_size=0.25,   # 0.25 of 0.8 = 0.2 overall
    stratify=y_tv,
    random_state=42
)

len(x_train), len(x_val), len(x_test)

def stylometric_features(text: str):
    text = str(text)
    # split into sentences by ., !, ?
    sentences = re.split(r"[.!?]+", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    words = text.split()

    n_sent = len(sentences) if sentences else 1
    n_words = len(words) if words else 1

    # sentence length stats
    sent_lens = [len(s.split()) for s in sentences] if sentences else [n_words]
    avg_sent_len = np.mean(sent_lens)
    std_sent_len = np.std(sent_lens)

    # type-token ratio
    unique_words = len(set(w.lower() for w in words))
    ttr = unique_words / n_words

    # long word ratio (>= 7 chars)
    long_words = sum(1 for w in words if len(w) >= 7)
    long_word_ratio = long_words / n_words

    # punctuation ratio
    punct_chars = sum(1 for c in text if c in string.punctuation)
    punct_ratio = punct_chars / max(len(text), 1)

    # uppercase ratio
    upper_chars = sum(1 for c in text if c.isupper())
    upper_ratio = upper_chars / max(len(text), 1)

    return np.array([
        avg_sent_len,
        std_sent_len,
        ttr,
        long_word_ratio,
        punct_ratio,
        upper_ratio,
    ])

# Precompute stylometric features
x_train_sty = np.vstack(x_train.apply(stylometric_features))
x_val_sty   = np.vstack(x_val.apply(stylometric_features))
x_test_sty  = np.vstack(x_test.apply(stylometric_features))

x_train_sty.shape

from transformers import AutoTokenizer, AutoModel

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

LM_NAME = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(LM_NAME)
bert_model = AutoModel.from_pretrained(LM_NAME).to(DEVICE)
bert_model.eval()

def encode_with_bert(texts, max_len=256, batch_size=16):
    all_embs = []
    texts = list(texts)
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        enc = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        ).to(DEVICE)

        with torch.no_grad():
            outputs = bert_model(**enc)
            hidden = outputs.last_hidden_state  # [B, T, H]
            emb = hidden.mean(dim=1)            # mean-pool over tokens -> [B, H]

        all_embs.append(emb.cpu().numpy())
    return np.concatenate(all_embs, axis=0)

x_train_bert = encode_with_bert(x_train.tolist())
x_val_bert   = encode_with_bert(x_val.tolist())
x_test_bert  = encode_with_bert(x_test.tolist())

x_train_bert.shape

val_results = []

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_word = TfidfVectorizer(
    ngram_range=(1, 2),
    max_features=20000,
    lowercase=True,
)

pipe_word_log = Pipeline([
    ("tfidf", tfidf_word),
    ("clf", LogisticRegression(max_iter=1000, n_jobs=-1))
])

pipe_word_log.fit(x_train, y_train)
y_val_pred = pipe_word_log.predict(x_val)
acc = accuracy_score(y_val, y_val_pred)
print("\n[1] Word TF-IDF + LogReg val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "word_tfidf_logreg",
    "acc": acc,
    "model": pipe_word_log,
    "kind": "word_tfidf"
})

pipe_word_svm = Pipeline([
    ("tfidf", tfidf_word),
    ("clf", LinearSVC())
])

pipe_word_svm.fit(x_train, y_train)
y_val_pred = pipe_word_svm.predict(x_val)
acc = accuracy_score(y_val, y_val_pred)
print("\n[2] Word TF-IDF + LinearSVM val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "word_tfidf_svm",
    "acc": acc,
    "model": pipe_word_svm,
    "kind": "word_tfidf"
})

tfidf_char = TfidfVectorizer(
    analyzer="char",
    ngram_range=(3, 5),
    max_features=30000,
    lowercase=True,
)

pipe_char_log = Pipeline([
    ("tfidf_char", tfidf_char),
    ("clf", LogisticRegression(max_iter=1000, n_jobs=-1))
])

pipe_char_log.fit(x_train, y_train)
y_val_pred = pipe_char_log.predict(x_val)
acc = accuracy_score(y_val, y_val_pred)
print("\n[3] Char TF-IDF + LogReg val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "char_tfidf_logreg",
    "acc": acc,
    "model": pipe_char_log,
    "kind": "char_tfidf"
})

pipe_char_svm = Pipeline([
    ("tfidf_char", tfidf_char),
    ("clf", LinearSVC())
])

pipe_char_svm.fit(x_train, y_train)
y_val_pred = pipe_char_svm.predict(x_val)
acc = accuracy_score(y_val, y_val_pred)
print("\n[4] Char TF-IDF + LinearSVM val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "char_tfidf_svm",
    "acc": acc,
    "model": pipe_char_svm,
    "kind": "char_tfidf"
})

pipe_sty_log = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=1000))
])

pipe_sty_log.fit(x_train_sty, y_train)
y_val_pred = pipe_sty_log.predict(x_val_sty)
acc = accuracy_score(y_val, y_val_pred)
print("\n[5] Stylometrics + LogReg val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "stylometrics_logreg",
    "acc": acc,
    "model": pipe_sty_log,
    "kind": "stylometric"
})

pipe_sty_svm = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LinearSVC())
])

pipe_sty_svm.fit(x_train_sty, y_train)
y_val_pred = pipe_sty_svm.predict(x_val_sty)
acc = accuracy_score(y_val, y_val_pred)
print("\n[6] Stylometrics + LinearSVM val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "stylometrics_svm",
    "acc": acc,
    "model": pipe_sty_svm,
    "kind": "stylometric"
})

pipe_bert_log = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000))
])

pipe_bert_log.fit(x_train_bert, y_train)
y_val_pred = pipe_bert_log.predict(x_val_bert)
acc = accuracy_score(y_val, y_val_pred)
print("\n[7] BERT linear probe + LogReg val acc:", acc)
print(classification_report(y_val, y_val_pred))
val_results.append({
    "name": "bert_linear_probe_logreg",
    "acc": acc,
    "model": pipe_bert_log,
    "kind": "bert_probe"
})

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

sty_tree = DecisionTreeClassifier(
    max_depth=5,
    random_state=42
)

sty_tree.fit(x_train_sty, y_train)
y_val_pred = sty_tree.predict(x_val_sty)
acc = accuracy_score(y_val, y_val_pred)
print("\n[8] Stylometrics + DecisionTree val acc:", acc)
print(classification_report(y_val, y_val_pred))

val_results.append({
    "name": "stylometrics_decision_tree",
    "acc": acc,
    "model": sty_tree,
    "kind": "stylometric"
})

sty_rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    n_jobs=-1,
    random_state=42
)

sty_rf.fit(x_train_sty, y_train)
y_val_pred = sty_rf.predict(x_val_sty)
acc = accuracy_score(y_val, y_val_pred)
print("\n[9] Stylometrics + RandomForest val acc:", acc)
print(classification_report(y_val, y_val_pred))

val_results.append({
    "name": "stylometrics_random_forest",
    "acc": acc,
    "model": sty_rf,
    "kind": "stylometric"
})

sty_gb = GradientBoostingClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)

sty_gb.fit(x_train_sty, y_train)
y_val_pred = sty_gb.predict(x_val_sty)
acc = accuracy_score(y_val, y_val_pred)
print("\n[10] Stylometrics + GradientBoosting val acc:", acc)
print(classification_report(y_val, y_val_pred))

val_results.append({
    "name": "stylometrics_gradient_boosting",
    "acc": acc,
    "model": sty_gb,
    "kind": "stylometric"
})

val_results_sorted = sorted(val_results, key=lambda d: d["acc"], reverse=True)

print("\n=== Validation results (sorted) ===")
for r in val_results_sorted:
    print(f"{r['name']:35s}  acc = {r['acc']:.4f}  ({r['kind']})")

best = val_results_sorted[0]
best_name = best["name"]
best_kind = best["kind"]
print("\nBest model on validation:", best_name, "acc =", best["acc"])

if best_kind in {"word_tfidf", "char_tfidf"}:
    print("\nRetraining best TF-IDF model on train+val...")
    best_model = best["model"]
    best_model.fit(x_tv, y_tv)
    y_test_pred = best_model.predict(x_test)
else:
    raise ValueError(f"Unknown best_kind: {best_kind}")

print("\n=== Final Test Performance (best model) ===")
print("Test accuracy:", accuracy_score(y_test, y_test_pred))
print(classification_report(y_test, y_test_pred))

ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred, cmap="Blues")
plt.title(f"Confusion Matrix – {best_name} (test)")
plt.show()

y_all_pred = best_model.predict(x)
ConfusionMatrixDisplay.from_predictions(y, y_all_pred, cmap="Blues")
plt.title("Confusion Matrix – full dataset (train+val+test)")
plt.show()